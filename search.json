[
  {
    "objectID": "xml.html",
    "href": "xml.html",
    "title": "xml source",
    "section": "",
    "text": "source\n\n\n\n json_to_xml (d:dict, rnm:str)\n\nConvert d to XML.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ndict\nJSON dictionary to convert\n\n\nrnm\nstr\nRoot name\n\n\nReturns\nstr\n\n\n\n\n\n\nExported source\ndef json_to_xml(d:dict, # JSON dictionary to convert\n                rnm:str # Root name\n               )-&gt;str:\n    \"Convert `d` to XML.\"\n    root = ET.Element(rnm)\n    def build_xml(data, parent):\n        if isinstance(data, dict):\n            for key, value in data.items(): build_xml(value, ET.SubElement(parent, key))\n        elif isinstance(data, list):\n            for item in data: build_xml(item, ET.SubElement(parent, 'item'))\n        else: parent.text = str(data)\n    build_xml(d, root)\n    ET.indent(root)\n    return ET.tostring(root, encoding='unicode')\n\n\nJSON doesn’t map as nicely to XML as the data structure used in fastcore.xml, but for simple XML trees it can be convenient – for example:\n\na = dict(surname='Howard', firstnames=['Jeremy','Peter'],\n         address=dict(state='Queensland',country='Australia'))\nhl_md(json_to_xml(a, 'person'))\n\n&lt;person&gt;\n  &lt;surname&gt;Howard&lt;/surname&gt;\n  &lt;firstnames&gt;\n    &lt;item&gt;Jeremy&lt;/item&gt;\n    &lt;item&gt;Peter&lt;/item&gt;\n  &lt;/firstnames&gt;\n  &lt;address&gt;\n    &lt;state&gt;Queensland&lt;/state&gt;\n    &lt;country&gt;Australia&lt;/country&gt;\n  &lt;/address&gt;\n&lt;/person&gt;",
    "crumbs": [
      "xml source"
    ]
  },
  {
    "objectID": "xml.html#setup",
    "href": "xml.html#setup",
    "title": "xml source",
    "section": "",
    "text": "source\n\n\n\n json_to_xml (d:dict, rnm:str)\n\nConvert d to XML.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ndict\nJSON dictionary to convert\n\n\nrnm\nstr\nRoot name\n\n\nReturns\nstr\n\n\n\n\n\n\nExported source\ndef json_to_xml(d:dict, # JSON dictionary to convert\n                rnm:str # Root name\n               )-&gt;str:\n    \"Convert `d` to XML.\"\n    root = ET.Element(rnm)\n    def build_xml(data, parent):\n        if isinstance(data, dict):\n            for key, value in data.items(): build_xml(value, ET.SubElement(parent, key))\n        elif isinstance(data, list):\n            for item in data: build_xml(item, ET.SubElement(parent, 'item'))\n        else: parent.text = str(data)\n    build_xml(d, root)\n    ET.indent(root)\n    return ET.tostring(root, encoding='unicode')\n\n\nJSON doesn’t map as nicely to XML as the data structure used in fastcore.xml, but for simple XML trees it can be convenient – for example:\n\na = dict(surname='Howard', firstnames=['Jeremy','Peter'],\n         address=dict(state='Queensland',country='Australia'))\nhl_md(json_to_xml(a, 'person'))\n\n&lt;person&gt;\n  &lt;surname&gt;Howard&lt;/surname&gt;\n  &lt;firstnames&gt;\n    &lt;item&gt;Jeremy&lt;/item&gt;\n    &lt;item&gt;Peter&lt;/item&gt;\n  &lt;/firstnames&gt;\n  &lt;address&gt;\n    &lt;state&gt;Queensland&lt;/state&gt;\n    &lt;country&gt;Australia&lt;/country&gt;\n  &lt;/address&gt;\n&lt;/person&gt;",
    "crumbs": [
      "xml source"
    ]
  },
  {
    "objectID": "xml.html#including-documents",
    "href": "xml.html#including-documents",
    "title": "xml source",
    "section": "Including documents",
    "text": "Including documents\nAccording to Anthropic, “it’s essential to structure your prompts in a way that clearly separates the input data from the instructions”. They recommend using something like the following:\nHere are some documents for you to reference for your task:\n    \n&lt;documents&gt;\n&lt;document index=\"1\"&gt;\n&lt;source&gt;\n(URL, file name, hash, etc)\n&lt;/source&gt;\n&lt;document_content&gt;\n(the text content)\n&lt;/document_content&gt;\n&lt;/document&gt;\n&lt;/documents&gt;\nWe will create some small helper functions to make it easier to generate context in this format, although we’re use &lt;src&gt; instead of &lt;source&gt; to avoid conflict with that HTML tag. Although it’s based on Anthropic’s recommendation, it’s likely to work well with other models too.\n\n\nExported source\ndoctype = namedtuple('doctype', ['src', 'content'])\n\n\nWe’ll use doctype to store our pairs.\n\n\nExported source\ndef _add_nls(s):\n    \"Add newlines to start and end of `s` if missing\"\n    if not s: return s\n    if s[ 0]!='\\n': s = '\\n'+s\n    if s[-1]!='\\n': s = s+'\\n'\n    return s\n\n\nSince Anthropic’s example shows newlines before and after each tag, we’ll do the same.\n\nto_xml(Src('a'))\n\n'&lt;src&gt;a&lt;/src&gt;'\n\n\n\nto_xml(Document('a'))\n\n'&lt;document&gt;a&lt;/document&gt;'\n\n\n\nto_xml(Documents('a'))\n\n'&lt;documents&gt;a&lt;/documents&gt;'\n\n\n\nsource\n\nmk_doctype\n\n mk_doctype (content:str, src:Optional[str]=None)\n\nCreate a doctype named tuple\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\nstr\n\nThe document content\n\n\nsrc\nOptional\nNone\nURL, filename, etc; defaults to md5(content) if not provided\n\n\nReturns\nnamedtuple\n\n\n\n\n\n\n\nExported source\ndef mk_doctype(content:str,  # The document content\n           src:Optional[str]=None # URL, filename, etc; defaults to `md5(content)` if not provided\n          ) -&gt; namedtuple:\n    \"Create a `doctype` named tuple\"\n    if src is None: src = hashlib.md5(content.encode()).hexdigest()[:8]\n    return doctype(_add_nls(str(src).strip()), _add_nls(content.strip()))\n\n\nThis is a convenience wrapper to ensure that a doctype has the needed information in the right format.\n\ndoc = 'This is a \"sample\"'\nmk_doctype(doc)\n\ndoctype(src='\\n47e19350\\n', content='\\nThis is a \"sample\"\\n')\n\n\n\nsource\n\n\nmk_doc\n\n mk_doc (index:int, content:str, src:Optional[str]=None, **kwargs)\n\nCreate an ft format tuple for a single doc in Anthropic’s recommended format\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nindex\nint\n\nThe document index\n\n\ncontent\nstr\n\nThe document content\n\n\nsrc\nOptional\nNone\nURL, filename, etc; defaults to md5(content) if not provided\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\ntuple\n\n\n\n\n\n\n\nExported source\ndef mk_doc(index:int,  # The document index\n           content:str,  # The document content\n           src:Optional[str]=None, # URL, filename, etc; defaults to `md5(content)` if not provided\n           **kwargs\n          ) -&gt; tuple:\n    \"Create an `ft` format tuple for a single doc in Anthropic's recommended format\"\n    dt = mk_doctype(content, src)\n    content = Document_content(NotStr(dt.content))\n    src = Src(NotStr(dt.src))\n    return Document(src, content, index=index, **kwargs)\n\n\nWe can now generate XML for one document in the suggested format:\n\nmk_doc(1, doc, title=\"test\")\n\n&lt;document index=\"1\" title=\"test\"&gt;&lt;src&gt;\n47e19350\n&lt;/src&gt;&lt;document-content&gt;\nThis is a \"sample\"\n&lt;/document-content&gt;&lt;/document&gt;\n\n\n\nsource\n\n\ndocs_xml\n\n docs_xml (docs:list[str], srcs:Optional[list]=None, prefix:bool=True,\n           details:Optional[list]=None)\n\nCreate an XML string containing docs in Anthropic’s recommended format\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndocs\nlist\n\nThe content of each document\n\n\nsrcs\nOptional\nNone\nURLs, filenames, etc; each one defaults to md5(content) if not provided\n\n\nprefix\nbool\nTrue\nInclude Anthropic’s suggested prose intro?\n\n\ndetails\nOptional\nNone\nOptional list of dicts with additional attrs for each doc\n\n\nReturns\nstr\n\n\n\n\n\n\n\nExported source\ndef docs_xml(docs:list[str],  # The content of each document\n             srcs:Optional[list]=None,  # URLs, filenames, etc; each one defaults to `md5(content)` if not provided\n             prefix:bool=True, # Include Anthropic's suggested prose intro?\n             details:Optional[list]=None # Optional list of dicts with additional attrs for each doc\n            )-&gt;str:\n    \"Create an XML string containing `docs` in Anthropic's recommended format\"\n    pre = 'Here are some documents for you to reference for your task:\\n\\n' if prefix else ''\n    if srcs is None: srcs = [None]*len(docs)\n    if details is None: details = [{}]*len(docs)\n    docs = (mk_doc(i+1, d, s, **kw) for i,(d,s,kw) in enumerate(zip(docs,srcs,details)))\n    return pre + to_xml(Documents(docs))\n\n\nPutting it all together, we have our final XML format:\n\ndocs = [doc, 'And another one']\nsrcs = [None, 'doc.txt']\nprint(docs_xml(docs, srcs))\n\nHere are some documents for you to reference for your task:\n\n&lt;documents&gt;&lt;document index=\"1\"&gt;&lt;src&gt;\n47e19350\n&lt;/src&gt;&lt;document-content&gt;\nThis is a \"sample\"\n&lt;/document-content&gt;&lt;/document&gt;&lt;document index=\"2\"&gt;&lt;src&gt;\ndoc.txt\n&lt;/src&gt;&lt;document-content&gt;\nAnd another one\n&lt;/document-content&gt;&lt;/document&gt;&lt;/documents&gt;",
    "crumbs": [
      "xml source"
    ]
  },
  {
    "objectID": "xml.html#context-creation",
    "href": "xml.html#context-creation",
    "title": "xml source",
    "section": "Context creation",
    "text": "Context creation\nNow that we can generate Anthropic’s XML format, let’s make it easy for a few common cases.\n\nFile list to context\nFor generating XML context from files, we’ll just read them as text and use the file names as src.\n\nsource\n\n\nfiles2ctx\n\n files2ctx (fnames:list[typing.Union[str,pathlib.Path]], prefix:bool=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfnames\nlist\n\nList of file names to add to context\n\n\nprefix\nbool\nTrue\nInclude Anthropic’s suggested prose intro?\n\n\nReturns\nstr\n\nXML for LM context\n\n\n\n\n\nExported source\ndef files2ctx(\n    fnames:list[Union[str,Path]], # List of file names to add to context\n    prefix:bool=True # Include Anthropic's suggested prose intro?\n)-&gt;str: # XML for LM context\n    fnames = [Path(o) for o in fnames]\n    contents = [o.read_text() for o in fnames]\n    return docs_xml(contents, fnames, prefix=prefix)\n\n\n\nfnames = ['samples/sample_core.py', 'samples/sample_styles.css']\nhl_md(files2ctx(fnames))\n\nHere are some documents for you to reference for your task:\n\n&lt;documents&gt;&lt;document index=\"1\"&gt;&lt;src&gt;\nsamples/sample_core.py\n&lt;/src&gt;&lt;document-content&gt;\nimport inspect\nempty = inspect.Parameter.empty\nmodels = 'claude-3-opus-20240229','claude-3-sonnet-20240229','claude-3-haiku-20240307'\n&lt;/document-content&gt;&lt;/document&gt;&lt;document index=\"2\"&gt;&lt;src&gt;\nsamples/sample_styles.css\n&lt;/src&gt;&lt;document-content&gt;\n.cell { margin-bottom: 1rem; }\n.cell &gt; .sourceCode { margin-bottom: 0; }\n.cell-output &gt; pre { margin-bottom: 0; }\n&lt;/document-content&gt;&lt;/document&gt;&lt;/documents&gt;\n\n\n\n\nFolder to context\n\nsource\n\n\nfolder2ctx\n\n folder2ctx (folder:Union[str,pathlib.Path], prefix:bool=True,\n             recursive:bool=True, symlinks:bool=True, file_glob:str=None,\n             file_re:str=None, folder_re:str=None,\n             skip_file_glob:str=None, skip_file_re:str=None,\n             skip_folder_re:str=None, func:callable=&lt;function join&gt;,\n             ret_folders:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfolder\nUnion\n\nFolder name containing files to add to context\n\n\nprefix\nbool\nTrue\nInclude Anthropic’s suggested prose intro?\n\n\nrecursive\nbool\nTrue\nsearch subfolders\n\n\nsymlinks\nbool\nTrue\nfollow symlinks?\n\n\nfile_glob\nstr\nNone\nOnly include files matching glob\n\n\nfile_re\nstr\nNone\nOnly include files matching regex\n\n\nfolder_re\nstr\nNone\nOnly enter folders matching regex\n\n\nskip_file_glob\nstr\nNone\nSkip files matching glob\n\n\nskip_file_re\nstr\nNone\nSkip files matching regex\n\n\nskip_folder_re\nstr\nNone\nSkip folders matching regex,\n\n\nfunc\ncallable\njoin\nfunction to apply to each matched file\n\n\nret_folders\nbool\nFalse\nreturn folders, not just files\n\n\nReturns\nstr\n\nXML for Claude context\n\n\n\n\n\nExported source\n@delegates(globtastic)\ndef folder2ctx(\n    folder:Union[str,Path], # Folder name containing files to add to context\n    prefix:bool=True, # Include Anthropic's suggested prose intro?\n    **kwargs # Passed to `globtastic`\n)-&gt;str: # XML for Claude context\n    fnames = globtastic(folder, **kwargs)\n    return files2ctx(fnames, prefix=prefix)\n\n\n\nprint(folder2ctx('samples', prefix=False, file_glob='*.py'))\n\n&lt;documents&gt;&lt;document index=\"1\"&gt;&lt;src&gt;\nsamples/sample_core.py\n&lt;/src&gt;&lt;document-content&gt;\nimport inspect\nempty = inspect.Parameter.empty\nmodels = 'claude-3-opus-20240229','claude-3-sonnet-20240229','claude-3-haiku-20240307'\n&lt;/document-content&gt;&lt;/document&gt;&lt;/documents&gt;\n\n\n\n\n\n\n\n\nTip\n\n\n\nAfter you install toolslm, folder2ctx becomes available from the command line. You can see how to use it with the following command:\nfolder2ctx -h",
    "crumbs": [
      "xml source"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Release notes",
    "section": "",
    "text": "New combined API (#38)\n\n\n\n\n\n\n\n\nAdd rm_fenced (#37)\n\n\n\n\n\nAdd edge case tests and fix them (#36)\n\n\n\n\n\n\n\n\nRemove object enumeration of tools (#35)\n\n\n\n\n\n\n\n\nHandle list and dict args to mk_ns (#34)\n\n\n\n\n\n\n\n\nAuto clean up bad param names in call_func (#33)\n\n\n\n\n\npython() function can’t be used as a tool (#32)\n\n\n\n\n\n\n\n\nOptionally dont raise error on call_func (#31), thanks to @erikgaas\ndict support in get_schema (#30)\n\n\n\n\n\n\n\n\nOptional libs (http2text, beautifulsoup, llms_txt) are no longer automatically installed\n\n\n\n\n\nLazily load optional modules (#29)\n\n\n\n\n\n\n\n\nPass glb,loc to python (#28)\n\n\n\n\n\n\n\n\nAdds call_func_async (#27), thanks to @mikonapoli\nAdd arg ignore links (#26), thanks to @Isaac-Flath\n\n\n\n\n\n\n\n\nAdd arg ignore links (#26), thanks to @Isaac-Flath\n\n\n\n\n\nfix: prevent markdown heading detection inside code blocks (#25), thanks to @franckalbinet\nFix markdown hierarchy parsing for arbitrary header levels (#22), thanks to @erikgaas\n\n\n\n\n\n\n\n\nReplace source with src in context generation (#17)\n\n\n\n\n\n\n\n\nEscape and print context in folder2ctx et al (#16)\n\n\n\n\n\n\n\n\nAdd dict2obj to md_hier funcs (#15)\nMigrate call_func from claudette to toolslm (#14), thanks to @ncoop57\nAllow for getting schemas from nested structures (#11), thanks to @ncoop57\nAllow for sel to select and wrap multiple element results (#10), thanks to @Isaac-Flath\n\n\n\n\n\nUsing get_schema on class method results in type missing error (#12)\n\n\n\n\n\n\n\n\nAdd read_docs and find_docs (#8)\n\n\n\n\n\n\n\n\nXML tools assume all files have content (#3)\n\n\n\n\n\n\nMinor updates\n\n\n\n\n\nRename project\n\n\n\n\n\nInitial alpha release"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Release notes",
    "section": "",
    "text": "New combined API (#38)"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "Release notes",
    "section": "",
    "text": "Add rm_fenced (#37)\n\n\n\n\n\nAdd edge case tests and fix them (#36)"
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "Release notes",
    "section": "",
    "text": "Remove object enumeration of tools (#35)"
  },
  {
    "objectID": "CHANGELOG.html#section-3",
    "href": "CHANGELOG.html#section-3",
    "title": "Release notes",
    "section": "",
    "text": "Handle list and dict args to mk_ns (#34)"
  },
  {
    "objectID": "CHANGELOG.html#section-4",
    "href": "CHANGELOG.html#section-4",
    "title": "Release notes",
    "section": "",
    "text": "Auto clean up bad param names in call_func (#33)\n\n\n\n\n\npython() function can’t be used as a tool (#32)"
  },
  {
    "objectID": "CHANGELOG.html#section-5",
    "href": "CHANGELOG.html#section-5",
    "title": "Release notes",
    "section": "",
    "text": "Optionally dont raise error on call_func (#31), thanks to @erikgaas\ndict support in get_schema (#30)"
  },
  {
    "objectID": "CHANGELOG.html#section-6",
    "href": "CHANGELOG.html#section-6",
    "title": "Release notes",
    "section": "",
    "text": "Optional libs (http2text, beautifulsoup, llms_txt) are no longer automatically installed\n\n\n\n\n\nLazily load optional modules (#29)"
  },
  {
    "objectID": "CHANGELOG.html#section-7",
    "href": "CHANGELOG.html#section-7",
    "title": "Release notes",
    "section": "",
    "text": "Pass glb,loc to python (#28)"
  },
  {
    "objectID": "CHANGELOG.html#section-8",
    "href": "CHANGELOG.html#section-8",
    "title": "Release notes",
    "section": "",
    "text": "Adds call_func_async (#27), thanks to @mikonapoli\nAdd arg ignore links (#26), thanks to @Isaac-Flath"
  },
  {
    "objectID": "CHANGELOG.html#section-9",
    "href": "CHANGELOG.html#section-9",
    "title": "Release notes",
    "section": "",
    "text": "Add arg ignore links (#26), thanks to @Isaac-Flath\n\n\n\n\n\nfix: prevent markdown heading detection inside code blocks (#25), thanks to @franckalbinet\nFix markdown hierarchy parsing for arbitrary header levels (#22), thanks to @erikgaas"
  },
  {
    "objectID": "CHANGELOG.html#section-10",
    "href": "CHANGELOG.html#section-10",
    "title": "Release notes",
    "section": "",
    "text": "Replace source with src in context generation (#17)"
  },
  {
    "objectID": "CHANGELOG.html#section-11",
    "href": "CHANGELOG.html#section-11",
    "title": "Release notes",
    "section": "",
    "text": "Escape and print context in folder2ctx et al (#16)"
  },
  {
    "objectID": "CHANGELOG.html#section-12",
    "href": "CHANGELOG.html#section-12",
    "title": "Release notes",
    "section": "",
    "text": "Add dict2obj to md_hier funcs (#15)\nMigrate call_func from claudette to toolslm (#14), thanks to @ncoop57\nAllow for getting schemas from nested structures (#11), thanks to @ncoop57\nAllow for sel to select and wrap multiple element results (#10), thanks to @Isaac-Flath\n\n\n\n\n\nUsing get_schema on class method results in type missing error (#12)"
  },
  {
    "objectID": "CHANGELOG.html#section-13",
    "href": "CHANGELOG.html#section-13",
    "title": "Release notes",
    "section": "",
    "text": "Add read_docs and find_docs (#8)"
  },
  {
    "objectID": "CHANGELOG.html#section-14",
    "href": "CHANGELOG.html#section-14",
    "title": "Release notes",
    "section": "",
    "text": "XML tools assume all files have content (#3)"
  },
  {
    "objectID": "CHANGELOG.html#section-15",
    "href": "CHANGELOG.html#section-15",
    "title": "Release notes",
    "section": "",
    "text": "Minor updates"
  },
  {
    "objectID": "CHANGELOG.html#section-16",
    "href": "CHANGELOG.html#section-16",
    "title": "Release notes",
    "section": "",
    "text": "Rename project"
  },
  {
    "objectID": "CHANGELOG.html#section-17",
    "href": "CHANGELOG.html#section-17",
    "title": "Release notes",
    "section": "",
    "text": "Initial alpha release"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "toolslm",
    "section": "",
    "text": "This is a work in progress…",
    "crumbs": [
      "toolslm"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "toolslm",
    "section": "Install",
    "text": "Install\npip install toolslm",
    "crumbs": [
      "toolslm"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "toolslm",
    "section": "How to use",
    "text": "How to use\n\nContext creation\ntoolslm has some helpers to make it easier to generate XML context from files, for instance folder2ctx:\n\nprint(folder2ctx('samples', prefix=False, file_glob='*.py'))\n\n&lt;documents&gt;&lt;document index=\"1\"&gt;&lt;src&gt;\nsamples/sample_core.py\n&lt;/src&gt;&lt;document-content&gt;\nimport inspect\nempty = inspect.Parameter.empty\nmodels = 'claude-3-opus-20240229','claude-3-sonnet-20240229','claude-3-haiku-20240307'\n&lt;/document-content&gt;&lt;/document&gt;&lt;/documents&gt;\n\n\nJSON doesn’t map as nicely to XML as the ft data structure from fastcore.xml, but for simple XML trees it can be convenient. The json_to_xml function handles that conversion:\n\na = dict(surname='Howard', firstnames=['Jeremy','Peter'],\n         address=dict(state='Queensland',country='Australia'))\nprint(json_to_xml(a, 'person'))\n\n&lt;person&gt;\n  &lt;surname&gt;Howard&lt;/surname&gt;\n  &lt;firstnames&gt;\n    &lt;item&gt;Jeremy&lt;/item&gt;\n    &lt;item&gt;Peter&lt;/item&gt;\n  &lt;/firstnames&gt;\n  &lt;address&gt;\n    &lt;state&gt;Queensland&lt;/state&gt;\n    &lt;country&gt;Australia&lt;/country&gt;\n  &lt;/address&gt;\n&lt;/person&gt;",
    "crumbs": [
      "toolslm"
    ]
  },
  {
    "objectID": "md_hier.html",
    "href": "md_hier.html",
    "title": "Markdown Hierarchy Parser",
    "section": "",
    "text": "from toolslm.md_hier import *\nfrom IPython.display import Markdown\nThe md_hier module provides utilities for parsing markdown documents and converting them into structured hierarchical dictionaries. This is particularly useful for processing documentation, extracting sections, or navigating complex markdown files programmatically.",
    "crumbs": [
      "Markdown Hierarchy Parser"
    ]
  },
  {
    "objectID": "md_hier.html#overview",
    "href": "md_hier.html#overview",
    "title": "Markdown Hierarchy Parser",
    "section": "Overview",
    "text": "Overview\nThe module provides a main function and supporting class: - create_heading_dict: Creates a nested dictionary structure matching the markdown hierarchy\n- HeadingDict: A dictionary-like object that also stores the markdown text content\nThe function handles code blocks properly by ignoring headings that appear within fenced code blocks.\n\n\ncreate_heading_dict\n\n create_heading_dict (text, rm_fenced=True)\n\nCreate a nested dictionary structure from markdown headings.\n\nsample_md = \"\"\"\n# Introduction\n\nWelcome to our documentation.\n\n## Getting Started\n\nFollow these steps to begin.\n\n### Installation\n\nRun the following command:\n\n```bash\n# Install the packackge\npip install our-package\n```\n\n### Configuration\n\nSet up your config file.\n\n## Advanced Usage\n\nFor advanced users only.\n\n# Appendix\n\nAdditional resources.\"\"\"\n\n\nresult = create_heading_dict(sample_md)\nprint(\"Available sections:\")\nfor key in result.keys(): print(f\"  {key}\")\nprint(f\"\\nRoot document has {len(result.text)} characters of text\")\n\nAvailable sections:\n  Introduction\n  Appendix\n\nRoot document has 328 characters of text\n\n\nYou can access any section’s content via the text attribute:\n\nprint(result['Introduction']['Getting Started']['Installation'].text)\n\n### Installation\n\nRun the following command:\n\n```bash\n# Install the packackge\n\n\nNotice how parent sections contain all their child content in their text attribute:\n\nprint(result['Introduction']['Getting Started'].text[:200] + \"…\")\n\n## Getting Started\n\nFollow these steps to begin.\n\n### Installation\n\nRun the following command:\n\n```bash\n# Install the packackge\npip install our-package\n```\n\n### Configuration…\n\n\ncreate_heading_dict creates a nested dictionary structure that mirrors the markdown hierarchy. Each heading becomes a dictionary key containing its subheadings.\n\nNested structure: Creates a tree-like dictionary hierarchy\nNavigation friendly: Easy to traverse programmatically\nCode block filtering: Removes code blocks before processing\n\nLet’s see the nested structure:\n\nresult = create_heading_dict(sample_md)\nprint(\"Structure:\")\nprint(f\"Root keys: {list(result.keys())}\")\nprint(f\"Introduction subkeys: {list(result['Introduction'].keys())}\")\nprint(f\"Getting Started subkeys: {list(result['Introduction']['Getting Started'].keys())}\")\n\nprint(f\"\\nType of result: {type(result)}\")\nprint(f\"Type of subsection: {type(result['Introduction'])}\")\nprint(f\"Has text attribute: {hasattr(result, 'text')}\")\n\nStructure:\nRoot keys: ['Introduction', 'Appendix']\nIntroduction subkeys: ['Getting Started', 'Advanced Usage']\nGetting Started subkeys: ['Installation', 'Configuration']\n\nType of result: &lt;class 'toolslm.md_hier.HeadingDict'&gt;\nType of subsection: &lt;class 'toolslm.md_hier.HeadingDict'&gt;\nHas text attribute: True\n\n\n\n\nBenefits\nThis approach provides the best of both worlds:\nStructure Navigation: Navigate the document hierarchy naturally using dictionary keys - result['Introduction']['Getting Started'] - Check section existence with 'section' in result - Iterate through subsections with result.keys()\nContent Access: Get the actual markdown text at any level - result.text - entire document\n- result['Introduction'].text - section with all subsections - result['Introduction']['Getting Started']['Installation'].text - specific subsection only\nUse Cases: - Documentation processing: Extract specific sections while preserving formatting - Content analysis: Analyze document structure and section lengths\n- Template generation: Build navigation interfaces from document structure - Section extraction: Pull out individual sections with their complete content",
    "crumbs": [
      "Markdown Hierarchy Parser"
    ]
  },
  {
    "objectID": "funccall.html",
    "href": "funccall.html",
    "title": "funccall source",
    "section": "",
    "text": "Exported source\nimport inspect\nfrom collections import abc\nfrom fastcore.utils import *\nfrom fastcore.docments import docments\nfrom typing import get_origin, get_args, Dict, List, Optional, Tuple, Union\nfrom types import UnionType",
    "crumbs": [
      "funccall source"
    ]
  },
  {
    "objectID": "funccall.html#function-calling",
    "href": "funccall.html#function-calling",
    "title": "funccall source",
    "section": "Function calling",
    "text": "Function calling\nMany LLMs do function calling (aka tool use) by taking advantage of JSON schema.\nWe’ll use docments to make getting JSON schema from Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. Here’s an example:\n\ndef silly_sum(\n    a:int, # First thing to sum\n    b:int=1, # Second thing to sum\n    c:list[int]=None, # A pointless argument\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    return a + b\n\nThis is what docments makes of that:\n\nd = docments(silly_sum, full=True)\nd\n\n{ 'a': { 'anno': &lt;class 'int'&gt;,\n         'default': &lt;class 'inspect._empty'&gt;,\n         'docment': 'First thing to sum'},\n  'b': {'anno': &lt;class 'int'&gt;, 'default': 1, 'docment': 'Second thing to sum'},\n  'c': {'anno': list[int], 'default': None, 'docment': 'A pointless argument'},\n  'return': { 'anno': &lt;class 'int'&gt;,\n              'default': &lt;class 'inspect._empty'&gt;,\n              'docment': 'The sum of the inputs'}}\n\n\nNote that this is an AttrDict so we can treat it like an object, or a dict:\n\nd.a.docment, d['a']['anno']\n\n('First thing to sum', int)\n\n\n\n\nExported source\ndef _types(t:type)-&gt;tuple[str,Optional[str]]:\n    \"Tuple of json schema type name and (if appropriate) array item name.\"\n    if t is empty: raise TypeError('Missing type')\n    tmap = {int:\"integer\", float:\"number\", str:\"string\", bool:\"boolean\", list:\"array\", dict:\"object\"}\n    tmap.update({k.__name__: v for k, v in tmap.items()})\n    if getattr(t, '__origin__', None) in (list,tuple):\n        args = getattr(t, '__args__', None)\n        item_type = \"object\" if not args else tmap.get(t.__args__[0].__name__, \"object\")\n        return \"array\", item_type\n    # if t is a string like 'int', directly use the string as the key\n    elif isinstance(t, str): return tmap.get(t, \"object\"), None\n    # if t is the type itself and a container\n    elif get_origin(t): return tmap.get(get_origin(t).__name__, \"object\"), None\n    # if t is the type itself like int, use the __name__ representation as the key\n    else: return tmap.get(t.__name__, \"object\"), None\n\n\nThis internal function is needed to convert Python types into JSON schema types.\n\n_types(list[int]), _types(int), _types('int')\n\n(('array', 'integer'), ('integer', None), ('integer', None))\n\n\n\n_types(List[int]), _types(Optional[str]), _types(str | None), _types(Tuple[str, int])\n\n(('array', 'integer'), ('object', None), ('object', None), ('array', 'string'))\n\n\nNote the current behavior:\n\nignores all but the first argument for tuples\nunion types map to object which is a stand-in for arbitrary types\n\nThese and other approximations may require further refinement in the future.\nWill also convert custom types to the object type.\n\nclass Custom: a: int\n_types(list[Custom]), _types(Custom)\n\n(('array', 'object'), ('object', None))\n\n\n\n\nExported source\ndef _param(name, info):\n    \"json schema parameter given `name` and `info` from docments full dict.\"\n    paramt,itemt = _types(info.anno)\n    pschema = dict(type=paramt, description=info.docment or \"\")\n    if itemt: pschema[\"items\"] = {\"type\": itemt}\n    if info.default is not empty: pschema[\"default\"] = info.default\n    return pschema\n\n\nThis private function converts a key/value pair from the docments structure into the dict that will be needed for the schema.\n\nn,o = first(d.items())\nprint(n,'//', o)\n_param(n, o)\n\na // {'docment': 'First thing to sum', 'anno': &lt;class 'int'&gt;, 'default': &lt;class 'inspect._empty'&gt;}\n\n\n{'type': 'integer', 'description': 'First thing to sum'}\n\n\n\n_handle_type(int, None), _handle_type(Path, None)\n\n({'type': 'integer'}, {'type': 'string', 'format': 'Path'})\n\n\n\nassert _is_parameterized(list[int]) == True\nassert _is_parameterized(int) == False\nassert _is_container(list[int]) == True\nassert _is_container(dict[str, int]) == True\nassert _is_container(int) == False\n\nFor union and optional types, Union covers older Union[str] syntax while UnionType covers 3.10+ str | None syntax.\n\ndef _example_new_unioin(opt_tup: str | None):\n    pass\n\nd = docments(_example_new_unioin, full=True)\nanno1 = first(d.items())[1].anno\n(anno1, get_origin(anno1), get_args(anno1))\n\n(str | None, types.UnionType, (str, NoneType))\n\n\n\ndef _example_old_union(opt_tup: Union[str, type(None)] =None):\n    pass\n\nd = docments(_example_old_union, full=True)\nanno2 = first(d.items())[1].anno\n(anno2, get_origin(anno2), get_args(anno2))\n\n(typing.Optional[str], typing.Union, (str, NoneType))\n\n\nSupport for both union types is part of the broader container handling:\n\n# Test primitive types\ndefs = {}\nassert _handle_type(int, defs) == {'type': 'integer'}\nassert _handle_type(str, defs) == {'type': 'string'}\nassert _handle_type(bool, defs) == {'type': 'boolean'}\nassert _handle_type(float, defs) == {'type': 'number'}\n\n# Test custom class\nclass TestClass:\n    def __init__(self, x: int, y: int): store_attr()\n\nresult = _handle_type(TestClass, defs)\nassert result == {'$ref': '#/$defs/TestClass'}\nassert 'TestClass' in defs\nassert defs['TestClass']['type'] == 'object'\nassert 'properties' in defs['TestClass']\n\n\n# Test primitive types in containers\nassert _handle_container(list, (int,), defs) == {'type': 'array', 'items': {'type': 'integer'}}\nassert _handle_container(tuple, (str,), defs) == {'type': 'array', 'items': {'type': 'string'}}\nassert _handle_container(set, (str,), defs) == {'type': 'array', 'items': {'type': 'string'}, 'uniqueItems': True}\nassert _handle_container(dict, (str,bool), defs) == {'type': 'object', 'additionalProperties': {'type': 'boolean'}}\n\nresult = _handle_container(list, (TestClass,), defs)\nassert result == {'type': 'array', 'items': {'$ref': '#/$defs/TestClass'}}\nassert 'TestClass' in defs\n\n# Test complex nested structure\nComplexType = dict[str, list[TestClass]]\nresult = _handle_container(dict, (str, list[TestClass]), defs)\nassert result == {\n    'type': 'object',\n    'additionalProperties': {\n        'type': 'array',\n        'items': {'$ref': '#/$defs/TestClass'}\n    }\n}\n\n\n# Test processing of a required integer property\nprops, req = {}, {}\nclass TestClass:\n    \"Test class\"\n    def __init__(\n        self,\n        x: int, # First thing\n        y: list[float], # Second thing\n        z: str = \"default\", # Third thing\n    ): store_attr()\n\nd = docments(TestClass, full=True)\n_process_property('x', d.x, props, req, defs)\nassert 'x' in props\nassert props['x']['type'] == 'integer'\nassert 'x' in req\n\n# Test processing of a required list property\n_process_property('y', d.y, props, req, defs)\nassert 'y' in props\nassert props['y']['type'] == 'array'\nassert props['y']['items']['type'] == 'number'\nassert 'y' in req\n\n# Test processing of an optional string property with default\n_process_property('z', d.z, props, req, defs)\nassert 'z' in props\nassert props['z']['type'] == 'string'\nassert props['z']['default'] == \"default\"\nassert 'z' not in req\n\n\nsource\n\nget_schema\n\n get_schema (f:Union[&lt;built-infunctioncallable&gt;,dict],\n             pname='input_schema')\n\nGenerate JSON schema for a class, function, or method\n\n\nExported source\ndef get_schema(f:Union[callable,dict], pname='input_schema')-&gt;dict:\n    \"Generate JSON schema for a class, function, or method\"\n    if isinstance(f, dict): return f\n    schema = _get_nested_schema(f)\n    desc = f.__doc__\n    assert desc, \"Docstring missing!\"\n    d = docments(f, full=True)\n    ret = d.pop('return')\n    if ret.anno is not empty: desc += f'\\n\\nReturns:\\n- type: {_types(ret.anno)[0]}'\n    return {\"name\": f.__name__, \"description\": desc, pname: schema}\n\n\nPutting this all together, we can now test getting a schema from silly_sum. The tool use spec doesn’t support return annotations directly, so we put that in the description instead.\n\ns = get_schema(silly_sum)\ndesc = s.pop('description')\nprint(desc)\ns\n\nAdds a + b.\n\nReturns:\n- type: integer\n\n\n{'name': 'silly_sum',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'},\n   'b': {'type': 'integer',\n    'description': 'Second thing to sum',\n    'default': 1},\n   'c': {'type': 'array',\n    'description': 'A pointless argument',\n    'items': {'type': 'integer'},\n    'default': None}},\n  'title': None,\n  'required': ['a']}}\n\n\nThis also works with string annotations, e.g:\n\ndef silly_test(\n    a: 'int',  # quoted type hint\n):\n    \"Mandatory docstring\"\n    return a\n\nget_schema(silly_test)\n\n{'name': 'silly_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'integer', 'description': 'quoted type hint'}},\n  'title': None,\n  'required': ['a']}}\n\n\nThis also works with instance methods:\n\nclass Dummy:\n    def sums(\n        self,\n        a:int,  # First thing to sum\n        b:int=1 # Second thing to sum\n    ) -&gt; int: # The sum of the inputs\n        \"Adds a + b.\"\n        print(f\"Finding the sum of {a} and {b}\")\n        return a + b\n\nget_schema(Dummy.sums)\n\n{'name': 'sums',\n 'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'},\n   'b': {'type': 'integer',\n    'description': 'Second thing to sum',\n    'default': 1}},\n  'title': None,\n  'required': ['a']}}\n\n\nget_schema also handles more complicated structures such as nested classes. This is useful for things like structured outputs.\n\nclass Turn:\n    \"Turn between two speakers\"\n    def __init__(\n        self,\n        speaker_a:str, # First speaker's message\n        speaker_b:str,  # Second speaker's message\n    ): store_attr()\n\nclass Conversation:\n    \"A conversation between two speakers\"\n    def __init__(\n        self,\n        turns:list[Turn], # Turns of the conversation\n    ): store_attr()\n\nget_schema(Conversation)\n\n{'name': 'Conversation',\n 'description': 'A conversation between two speakers',\n 'input_schema': {'type': 'object',\n  'properties': {'turns': {'type': 'array',\n    'description': 'Turns of the conversation',\n    'items': {'$ref': '#/$defs/Turn'}}},\n  'title': 'Conversation',\n  'required': ['turns'],\n  '$defs': {'Turn': {'type': 'object',\n    'properties': {'speaker_a': {'type': 'string',\n      'description': \"First speaker's message\"},\n     'speaker_b': {'type': 'string',\n      'description': \"Second speaker's message\"}},\n    'title': 'Turn',\n    'required': ['speaker_a', 'speaker_b']}}}}\n\n\n\nclass DictConversation:\n    \"A conversation between two speakers\"\n    def __init__(\n        self,\n        turns:dict[str,list[Turn]], # dictionary of topics and the Turns of the conversation\n    ): store_attr()\n\nget_schema(DictConversation)\n\n{'name': 'DictConversation',\n 'description': 'A conversation between two speakers',\n 'input_schema': {'type': 'object',\n  'properties': {'turns': {'type': 'object',\n    'description': 'dictionary of topics and the Turns of the conversation',\n    'additionalProperties': {'type': 'array',\n     'items': {'$ref': '#/$defs/Turn'}}}},\n  'title': 'DictConversation',\n  'required': ['turns'],\n  '$defs': {'Turn': {'type': 'object',\n    'properties': {'speaker_a': {'type': 'string',\n      'description': \"First speaker's message\"},\n     'speaker_b': {'type': 'string',\n      'description': \"Second speaker's message\"}},\n    'title': 'Turn',\n    'required': ['speaker_a', 'speaker_b']}}}}\n\n\n\nclass SetConversation:\n    \"A conversation between two speakers\"\n    def __init__(\n        self,\n        turns:set[Turn], # the unique Turns of the conversation\n    ): store_attr()\n\nget_schema(SetConversation)\n\n{'name': 'SetConversation',\n 'description': 'A conversation between two speakers',\n 'input_schema': {'type': 'object',\n  'properties': {'turns': {'type': 'array',\n    'description': 'the unique Turns of the conversation',\n    'items': {'$ref': '#/$defs/Turn'},\n    'uniqueItems': True}},\n  'title': 'SetConversation',\n  'required': ['turns'],\n  '$defs': {'Turn': {'type': 'object',\n    'properties': {'speaker_a': {'type': 'string',\n      'description': \"First speaker's message\"},\n     'speaker_b': {'type': 'string',\n      'description': \"Second speaker's message\"}},\n    'title': 'Turn',\n    'required': ['speaker_a', 'speaker_b']}}}}\n\n\n\nsource\n\n\nPathArg\n\n PathArg (path:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\nA filesystem path\n\n\n\n\n\nExported source\ndef PathArg(\n    path: str  # A filesystem path\n): return Path(path)\n\n\nPaths are a special case, since they only take *args and **kwargs as params, but normally we’d use them in a schema by just passing a str. So we create a custom param type for that.\n\ndef path_test(\n    a: PathArg,  # a type hint\n    b: PathArg   # b type hint\n):\n    \"Mandatory docstring\"\n    return a/b\n\nget_schema(path_test)\n\n{'name': 'path_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'object',\n    'description': 'a type hint',\n    '$ref': '#/$defs/PathArg'},\n   'b': {'type': 'object',\n    'description': 'b type hint',\n    '$ref': '#/$defs/PathArg'}},\n  'title': None,\n  'required': ['a', 'b'],\n  '$defs': {'PathArg': {'type': 'object',\n    'properties': {'path': {'type': 'string',\n      'description': 'A filesystem path'}},\n    'title': None,\n    'required': ['path']}}}}\n\n\nAlternatively, use Path as usual, and handle the format key in the json to use that as a callable:\n\ndef path_test2(\n    a: Path,  # a type hint\n    b: Path   # b type hint\n):\n    \"Mandatory docstring\"\n    return a/b\n\nget_schema(path_test2)\n\n{'name': 'path_test2',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'string',\n    'description': 'a type hint',\n    'format': 'Path'},\n   'b': {'type': 'string', 'description': 'b type hint', 'format': 'Path'}},\n  'title': None,\n  'required': ['a', 'b']}}\n\n\n\n\nAdditional get_schema() Test Cases\nUnion types are approximately mapped to JSON schema ‘anyOf’ with two or more value types.\n\ndef _union_test(opt_tup: Union[Tuple[int, int], str, int]=None):\n    \"Mandatory docstring\"\n    return \"\"\nget_schema(_union_test)\n\n{'name': '_union_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'opt_tup': {'type': 'object',\n    'description': '',\n    'default': None,\n    'anyOf': [{'type': 'array'}, {'type': 'string'}, {'type': 'integer'}]}},\n  'title': None}}\n\n\nThe new (Python 3.10+) union syntax can also be used, producing an equivalent schema.\n\ndef _new_union_test(opt_tup: Tuple[int, int] | str | int =None):\n    \"Mandatory docstring\"\n    pass\nget_schema(_new_union_test)\n\n{'name': '_new_union_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'opt_tup': {'type': 'object',\n    'description': '',\n    'default': None,\n    'anyOf': [{'type': 'array'}, {'type': 'string'}, {'type': 'integer'}]}},\n  'title': None}}\n\n\nOptional is a special case of union types, limited to two types, one of which is None (mapped to null in JSON schema):\n\ndef _optional_test(opt_tup: Optional[Tuple[int, int]]=None):\n    \"Mandatory docstring\"\n    pass\nget_schema(_optional_test)\n\n{'name': '_optional_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'opt_tup': {'type': 'object',\n    'description': '',\n    'default': None,\n    'anyOf': [{'type': 'array'}, {'type': 'null'}]}},\n  'title': None}}\n\n\nContainers can also be used, both in their parameterized form (List[int]) or as their unparameterized raw type (List). In the latter case, the item type is mapped to object in JSON schema.\n\ndef _list_test(l: List[int]):\n    \"Mandatory docstring\"\n    pass\nget_schema(_list_test)\n\n{'name': '_list_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'l': {'type': 'array',\n    'description': '',\n    'items': {'type': 'integer'}}},\n  'title': None,\n  'required': ['l']}}\n\n\n\ndef _raw_list_test(l: List):\n    \"Mandatory docstring\"\n    pass\nget_schema(_raw_list_test)\n\n{'name': '_raw_list_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'l': {'type': 'array',\n    'description': '',\n    'items': {'type': 'object'}}},\n  'title': None,\n  'required': ['l']}}\n\n\nThe same applies to dictionary, which can similarly be parameterized with key/value types or specified as a raw type.\n\ndef _dict_test(d: Dict[str, int]):\n    \"Mandatory docstring\"\n    pass\nget_schema(_dict_test)\n\n{'name': '_dict_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'d': {'type': 'object',\n    'description': '',\n    'additionalProperties': {'type': 'integer'}}},\n  'title': None,\n  'required': ['d']}}\n\n\n\ndef _raw_dict_test(d: Dict):\n    \"Mandatory docstring\"\nget_schema(_raw_dict_test)\n\n{'name': '_raw_dict_test',\n 'description': 'Mandatory docstring',\n 'input_schema': {'type': 'object',\n  'properties': {'d': {'type': 'object', 'description': ''}},\n  'title': None,\n  'required': ['d']}}\n\n\n\n\nPython tool\nIn language model clients it’s often useful to have a ‘code interpreter’ – this is something that runs code, and generally outputs the result of the last expression (i.e like IPython or Jupyter).\nIn this section we’ll create the python function, which executes a string as Python code, with an optional timeout. If the last line is an expression, we’ll return that – just like in IPython or Jupyter, but without needing them installed.\n\n\nExported source\nimport ast, time, signal, traceback\nfrom fastcore.utils import *\n\n\n\n\nExported source\ndef _copy_loc(new, orig):\n    \"Copy location information from original node to new node and all children.\"\n    new = ast.copy_location(new, orig)\n    for field, o in ast.iter_fields(new):\n        if isinstance(o, ast.AST): setattr(new, field, _copy_loc(o, orig))\n        elif isinstance(o, list): setattr(new, field, [_copy_loc(value, orig) for value in o])\n    return new\n\n\nThis is an internal function that’s needed for _run to ensure that location information is available in the abstract syntax tree (AST), since otherwise python complains.\n\n\nExported source\ndef _run(code:str, glb:dict=None, loc:dict=None):\n    \"Run `code`, returning final expression (similar to IPython)\"\n    tree = ast.parse(code)\n    last_node = tree.body[-1] if tree.body else None\n    \n    # If the last node is an expression, modify the AST to capture the result\n    if isinstance(last_node, ast.Expr):\n        tgt = [ast.Name(id='_result', ctx=ast.Store())]\n        assign_node = ast.Assign(targets=tgt, value=last_node.value)\n        tree.body[-1] = _copy_loc(assign_node, last_node)\n\n    compiled_code = compile(tree, filename='&lt;ast&gt;', mode='exec')\n    glb = glb or {}\n    stdout_buffer = io.StringIO()\n    saved_stdout = sys.stdout\n    sys.stdout = stdout_buffer\n    try: exec(compiled_code, glb, loc)\n    finally: sys.stdout = saved_stdout\n    _result = glb.get('_result', None)\n    if _result is not None: return _result\n    return stdout_buffer.getvalue().strip()\n\n\nThis is the internal function used to actually run the code – we pull off the last AST to see if it’s an expression (i.e something that returns a value), and if so, we store it to a special _result variable so we can return it.\n\n_run('import math;math.factorial(12)')\n\n479001600\n\n\n\n_run('print(1+1)')\n\n'2'\n\n\nWe now have the machinery needed to create our python function.\n\nsource\n\n\npython\n\n python (code:str, glb:Optional[dict]=None, loc:Optional[dict]=None,\n         timeout:int=3600)\n\nExecutes python code with timeout and returning final expression (similar to IPython).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncode\nstr\n\nCode to execute\n\n\nglb\nOptional\nNone\nGlobals namespace\n\n\nloc\nOptional\nNone\nLocals namespace\n\n\ntimeout\nint\n3600\nMaximum run time in seconds\n\n\n\n\n\nExported source\ndef python(\n    code:str, # Code to execute\n    glb:Optional[dict]=None, # Globals namespace\n    loc:Optional[dict]=None, # Locals namespace\n    timeout:int=3600 # Maximum run time in seconds\n):\n    \"Executes python `code` with `timeout` and returning final expression (similar to IPython).\"\n    def handler(*args): raise TimeoutError()\n    if glb is None: glb = inspect.currentframe().f_back.f_globals\n    if loc is None: loc=glb\n    signal.signal(signal.SIGALRM, handler)\n    signal.alarm(timeout)\n    try: return _run(code, glb, loc)\n    except Exception as e: return traceback.format_exc()\n    finally: signal.alarm(0)\n\n\nThere’s no builtin security here – you should generally use this in a sandbox, or alternatively prompt before running code. It can handle multiline function definitions, and pretty much any other normal Python syntax.\n\npython(\"\"\"def factorial(n):\n    if n == 0 or n == 1: return 1\n    else: return n * factorial(n-1)\nfactorial(5)\"\"\")\n\n120\n\n\nIf the code takes longer than timeout then it returns an error string.\n\nprint(python('import time; time.sleep(10)', timeout=1))\n\nTraceback (most recent call last):\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_10039/2052945749.py\", line 14, in python\n    try: return _run(code, glb, loc)\n                ^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_10039/1858893181.py\", line 18, in _run\n    try: exec(compiled_code, glb, loc)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;ast&gt;\", line 1, in &lt;module&gt;\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_10039/2052945749.py\", line 9, in handler\n    def handler(*args): raise TimeoutError()\n                        ^^^^^^^^^^^^^^^^^^^^\nTimeoutError\n\n\n\nBy default the caller’s global namespace is used.\n\npython(\"a=1\")\na\n\n1\n\n\nPass a different glb if needed; this requires using python_ns.\n\nglb = {}\npython(\"a=3\", glb=glb)\na, glb['a']\n\n(1, 3)\n\n\n\nget_schema(python)\n\n{'name': 'python',\n 'description': 'Executes python `code` with `timeout` and returning final expression (similar to IPython).',\n 'input_schema': {'type': 'object',\n  'properties': {'code': {'type': 'string', 'description': 'Code to execute'},\n   'glb': {'type': 'object',\n    'description': 'Globals namespace',\n    'default': None,\n    'anyOf': [{'type': 'object'}, {'type': 'null'}]},\n   'loc': {'type': 'object',\n    'description': 'Locals namespace',\n    'default': None,\n    'anyOf': [{'type': 'object'}, {'type': 'null'}]},\n   'timeout': {'type': 'integer',\n    'description': 'Maximum run time in seconds',\n    'default': 3600}},\n  'title': None,\n  'required': ['code']}}\n\n\n\n\nTool Calling\nMany LLM API providers offer tool calling where an LLM can choose to call a given tool. This is also helpful for structured outputs since the response from the LLM is contrained to the required arguments of the tool.\nThis section will be dedicated to helper functions for calling tools. We don’t want to allow LLMs to call just any possible function (that would be a security disaster!) so we create a namespace – that is, a dictionary of allowable function names to call.\n\nsource\n\n\nmk_ns\n\n mk_ns (fs)\n\n\ndef sums(a, b): return a + b\nns = mk_ns(sums); ns\n\n{'sums': &lt;function __main__.sums(a, b)&gt;}\n\n\n\nns['sums'](1, 2)\n\n3\n\n\n\nsource\n\n\ncall_func\n\n call_func (fc_name, fc_inputs, ns, raise_on_err=True)\n\nCall the function fc_name with the given fc_inputs using namespace ns.\n\n\nExported source\ndef call_func(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Call the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n    if not isinstance(ns, abc.Mapping): ns = mk_ns(ns)\n    func = ns[fc_name]\n    # Clean up bad param names\n    inps = {re.sub(r'\\W', '', k):v for k,v in fc_inputs.items()}\n    try: return func(**fc_inputs)\n    except Exception as e:\n        if raise_on_err: raise e from None\n        else: return traceback.format_exc()\n\n\nNow when we an LLM responses with the tool to use and its inputs, we can simply use the same namespace it was given to look up the tool and call it.\n\ncall_func('sums', {'a': 1, 'b': 2}, ns=[sums])\n\n3\n\n\n\nassert \"unsupported operand type(s) for +: 'int' and 'str'\" in call_func('sums', {'a': 1, 'b': '3'}, ns=ns, raise_on_err=False)\n\n\ntest_fail(call_func, args=['sums', {'a': 1, 'b': '3'}], kwargs={'ns': ns})\n\n\n\nAsync function calling\n\nasync def asums(a, b): return a + b\nns = mk_ns(asums); ns\n\n{'asums': &lt;function __main__.asums(a, b)&gt;}\n\n\n\nsource\n\n\ncall_func_async\n\n call_func_async (fc_name, fc_inputs, ns, raise_on_err=True)\n\nAwaits the function fc_name with the given fc_inputs using namespace ns.\n\n\nExported source\nasync def call_func_async(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Awaits the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n    res = call_func(fc_name, fc_inputs, ns, raise_on_err=raise_on_err)\n    if inspect.iscoroutine(res):\n        try: res = await res\n        except Exception as e:\n            if raise_on_err: raise e from None\n            else: return traceback.format_exc()\n    return res\n\n\n\nawait call_func_async('asums', {'a': 1, 'b': 2}, ns=[asums])\n\n3\n\n\n\nr = await call_func_async('asums', {'a': 1, 'b': '2'}, ns=[asums], raise_on_err=False)\nassert \"unsupported operand type(s) for +: 'int' and 'str'\" in r\n\n\nex = False\ntry: await call_func_async('asums', {'a': 1, 'b': '2'}, ns=[asums], raise_on_err=True)\nexcept: ex = True\nassert ex",
    "crumbs": [
      "funccall source"
    ]
  },
  {
    "objectID": "shell.html",
    "href": "shell.html",
    "title": "shell source",
    "section": "",
    "text": "Exported source\nimport ast, time, signal, traceback\nfrom fastcore.utils import *\n\n\nget_shell is like python, except it also maintains a stateful interpreter, rather than just running a single line of code. This is implemented using IPython, so that must be installed.\n\n\nExported source\nfrom IPython.terminal.interactiveshell import TerminalInteractiveShell\nfrom IPython.utils.capture import capture_output\n\n\n\ndef exception2str(ex:Exception)-&gt;str:\n    \"Convert exception `ex` into a string\"\n    return ''.join(traceback.format_exception(type(ex), ex, ex.__traceback__))\n\n\ntry: print(1/0)\nexcept Exception as e: print(exception2str(e))\n\nTraceback (most recent call last):\n  File \"/var/folders/ss/34z569j921v58v8n1n_8z7h40000gn/T/ipykernel_37260/4058275565.py\", line 1, in &lt;module&gt;\n    try: print(1/0)\n               ~^~\nZeroDivisionError: division by zero\n\n\n\n\nsource\n\nTerminalInteractiveShell.run_cell\n\n TerminalInteractiveShell.run_cell (cell, timeout=None)\n\nWrapper for original run_cell which adds timeout and output capture\n\n\nExported source\nTerminalInteractiveShell.orig_run = TerminalInteractiveShell.run_cell\n\n\n\n\nExported source\n@patch\ndef run_cell(self:TerminalInteractiveShell, cell, timeout=None):\n    \"Wrapper for original `run_cell` which adds timeout and output capture\"\n    if timeout:\n        def handler(*args): raise TimeoutError()\n        signal.signal(signal.SIGALRM, handler)\n        signal.alarm(timeout)\n    try:\n        with capture_output() as io: result = self.orig_run(cell)\n        result.stdout = io.stdout\n        return result\n    except TimeoutException as e:\n        result = self.ExecutionResult(error_before_exec=None, error_in_exec=e)\n    finally:\n        if timeout: signal.alarm(0)\n\n\n\nsource\n\n\nget_shell\n\n get_shell ()\n\nGet a TerminalInteractiveShell with minimal functionality\n\n\nExported source\ndef get_shell()-&gt;TerminalInteractiveShell:\n    \"Get a `TerminalInteractiveShell` with minimal functionality\"\n    sh = TerminalInteractiveShell()\n    sh.logger.log_output = sh.history_manager.enabled = False\n    dh = sh.displayhook\n    dh.finish_displayhook = dh.write_output_prompt = dh.start_displayhook = lambda: None\n    dh.write_format_data = lambda format_dict, md_dict=None: None\n    sh.logstart = sh.automagic = sh.autoindent = False\n    sh.autocall = 0\n    sh.system = lambda cmd: None\n    return sh\n\n\n\nshell = get_shell()\n\n\nr = shell.run_cell('print(3); 1+1')\nr.result,r.stdout\n\n(2, '3\\n')\n\n\n\nr = shell.run_cell('raise Exception(\"blah\")')\nprint(exception2str(r.error_in_exec))\n\nTraceback (most recent call last):\n  File \"/Users/jhoward/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-1-338156281413&gt;\", line 1, in &lt;module&gt;\n    raise Exception(\"blah\")\nException: blah\n\n\n\n\nr = shell.run_cell('import time; time.sleep(10)', timeout=1)\nr.error_in_exec\n\nTimeoutError()",
    "crumbs": [
      "shell source"
    ]
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download helpers",
    "section": "",
    "text": "from IPython.display import Markdown,HTML\nfrom fastcore.test import *\n\n\nsource\n\nclean_md\n\n clean_md (text, rm_comments=True, rm_details=True)\n\nRemove comments and &lt;details&gt; sections from text\n\nsource\n\n\nread_md\n\n read_md (url, rm_comments=True, rm_details=True,\n          params:QueryParamTypes|None=None, headers:HeaderTypes|None=None,\n          cookies:CookieTypes|None=None, auth:AuthTypes|None=None,\n          proxy:ProxyTypes|None=None, follow_redirects:bool=False,\n          verify:ssl.SSLContext|str|bool=True,\n          timeout:TimeoutTypes=Timeout(timeout=5.0), trust_env:bool=True)\n\nRead text from url and clean with clean_docs\n\nmdurl = 'https://claudette.answer.ai/index.html.md'\nmd = read_md(mdurl)\n# Markdown(md)\n\n\nsource\n\n\nhtml2md\n\n html2md (s:str, ignore_links=True)\n\nConvert s from HTML to markdown\n\nsource\n\n\nread_html\n\n read_html (url, sel=None, rm_comments=True, rm_details=True, multi=False,\n            wrap_tag=None, ignore_links=True)\n\nGet url, optionally selecting CSS selector sel, and convert to clean markdown\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\n\n\nURL to read\n\n\nsel\nNoneType\nNone\nRead only outerHTML of CSS selector sel\n\n\nrm_comments\nbool\nTrue\nRemoves HTML comments\n\n\nrm_details\nbool\nTrue\nRemoves &lt;details&gt; tags\n\n\nmulti\nbool\nFalse\nGet all matches to sel or first one\n\n\nwrap_tag\nNoneType\nNone\nIf multi, each selection wrapped with content\n\n\nignore_links\nbool\nTrue\n\n\n\n\n\n# test single class selector\nlistings = read_html('https://www.answer.ai/', sel='.listing-description')\nassert len(listings) &lt; 500\n\n# Test multi class selector\nlistings = read_html('https://www.answer.ai/', sel='.listing-description', multi=True)\nassert len(listings) &gt; 1000 # returns more than single so selecting multi\n\n# Test multi_wrap_tag\nlistings = read_html('https://www.answer.ai/', sel='.listing-description', multi=True, wrap_tag='document')\nassert '&lt;document&gt;' in listings and '&lt;/document&gt;' in listings\n\n\nread_html('https://www.answer.ai/', sel='.listing-description', ignore_links=False)\n\n'[My experience learning GPU programming, and implementing a new GPU education app in the process](./posts/2025-03-17-gpu-programming-scratch.html)\\n\\n'\n\n\n\n# test tag css selectors\nassert len(read_html('https://www.answer.ai/', sel='div.listing-description', multi=True)) &gt; 1000\nassert len(read_html('https://www.answer.ai/', sel='div', multi=True)) &gt; 1000\n\n\nhtmlurl = 'https://hypermedia.systems/hypermedia-a-reintroduction/'\nhmd = read_html(htmlurl)\nassert len(hmd) &gt; 100\n# Markdown(hmd)\n\n\nsource\n\n\nget_llmstxt\n\n get_llmstxt (url, optional=False, n_workers=None)\n\nGet llms.txt file from and expand it with llms_txt.create_ctx()\n\n# print(get_llmstxt('https://llmstxt.org/llms.txt'))\n\n\nsource\n\n\nsplit_url\n\n split_url (url)\n\nSplit url into base, path, and file name, normalising name to ‘/’ if empty\n\nurls = ('https://claudette.answer.ai/path/', 'https://claudette.answer.ai/', 'https://llmstxt.org', 'https://llmstxt.org/')\n\n[split_url(o) for o in urls]\n\n[('https://claudette.answer.ai', '', '/path'),\n ('https://claudette.answer.ai', '/', ''),\n ('https://llmstxt.org', '/', ''),\n ('https://llmstxt.org', '/', '')]\n\n\n\nsource\n\n\nfind_docs\n\n find_docs (url)\n\nIf available, return LLM-friendly llms.txt context or markdown file location from url\n\nfl_url = 'https://answerdotai.github.io/fastlite'\n\n\nfind_docs(fl_url)\n\n'https://answerdotai.github.io/fastlite/llms.txt'\n\n\n\nfor o in urls: print(find_docs(o))\n\nhttps://claudette.answer.ai/llms.txt\nhttps://claudette.answer.ai/llms.txt\nhttps://llmstxt.org/llms.txt\nhttps://llmstxt.org/llms.txt\n\n\n\nsuffixes = [\"/\", \"/tmp\", \"/tmp/tmp/\"]\nfor suff in suffixes:\n    for o in urls:  test_eq(find_docs(o), find_docs(o+suff))\n\ntest_eq(find_docs(\"https://github.com\"), \"https://github.com/llms.txt\")\ntest_eq(find_docs(\"https://github.com/AnswerDotAI\"), \"https://github.com/llms.txt\")\ntest_eq(find_docs(\"https://github.com/AnswerDotAI/\"), \"https://github.com/llms.txt\")\n\n\nsource\n\n\nread_docs\n\n read_docs (url, optional=False, n_workers=None, rm_comments=True,\n            rm_details=True)\n\nIf available, return LLM-friendly llms.txt context or markdown file response for url",
    "crumbs": [
      "Download helpers"
    ]
  }
]