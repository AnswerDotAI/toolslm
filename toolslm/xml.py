# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_xml.ipynb.

# %% auto 0
__all__ = ['doctype', 'json_to_xml', 'get_mime_text', 'cell2out', 'cell2xml', 'nb2xml', 'mk_doctype', 'mk_doc', 'docs_xml',
           'read_file', 'files2ctx', 'folder2ctx', 'sym2file', 'sym2folderctx', 'sym2pkgpath', 'sym2pkgctx',
           'folder2ctx_cli', 'parse_gh_url', 'repo2ctx']

# %% ../00_xml.ipynb
import hashlib, inspect, xml.etree.ElementTree as ET
from collections import namedtuple
from ghapi.all import GhApi

from fastcore.utils import *
from fastcore.meta import delegates
from fastcore.xtras import hl_md
from fastcore.xml import to_xml, Document, Documents, Document_content, Src, Source,Out,Outs,Cell,Notebook,Md,Code
from fastcore.script import call_parse

# %% ../00_xml.ipynb
def json_to_xml(d:dict, # JSON dictionary to convert
                rnm:str # Root name
               )->str:
    "Convert `d` to XML."
    root = ET.Element(rnm)
    def build_xml(data, parent):
        if isinstance(data, dict):
            for key, value in data.items(): build_xml(value, ET.SubElement(parent, key))
        elif isinstance(data, list):
            for item in data: build_xml(item, ET.SubElement(parent, 'item'))
        else: parent.text = str(data)
    build_xml(d, root)
    ET.indent(root)
    return ET.tostring(root, encoding='unicode')

# %% ../00_xml.ipynb
def get_mime_text(data):
    "Get text from MIME bundle, preferring markdown over plain"
    if 'text/markdown' in data: return ''.join(list(data['text/markdown']))
    if 'text/plain' in data: return ''.join(list(data['text/plain']))

# %% ../00_xml.ipynb
def cell2out(o):
    "Convert single notebook output to XML format"
    if hasattr(o, 'data'): 
        txt = get_mime_text(o.data)
        if txt: return Out(txt, mime='markdown' if 'text/markdown' in o.data else 'plain')
    if hasattr(o, 'text'):
        txt = o.text if isinstance(o.text, str) else ''.join(o.text)
        return Out(txt, type='stream', name=o.get('name', 'stdout'))
    if hasattr(o, 'ename'): return Out(f"{o.ename}: {o.evalue}", type='error')

# %% ../00_xml.ipynb
def cell2xml(cell, out=True):
    "Convert notebook cell to concise XML format"
    src = ''.join(getattr(cell, 'source', ''))
    f = Code if cell.cell_type=='code' else Md
    if not out: return f(src)
    parts = [Source(src)]
    out_items = L(getattr(cell,'outputs',[])).map(cell2out).filter()
    if out_items: parts.append(Outs(*out_items))
    return f(*parts)

# %% ../00_xml.ipynb
def nb2xml(fname=None, nb=None, out=True):
    "Convert notebook to XML format"
    assert bool(fname)^bool(nb), "Pass either `fname` or `nb`"
    if not nb: nb = dict2obj(fname.read_json())
    cells_xml = [to_xml(cell2xml(c, out=out), do_escape=False) for c in nb.cells if c.cell_type in ('code','markdown')]
    return to_xml(Notebook(*cells_xml), do_escape=False)

# %% ../00_xml.ipynb
doctype = namedtuple('doctype', ['src', 'content'])

# %% ../00_xml.ipynb
def _add_nls(s):
    "Add newlines to start and end of `s` if missing"
    if not s: return s
    if s[ 0]!='\n': s = '\n'+s
    if s[-1]!='\n': s = s+'\n'
    return s

# %% ../00_xml.ipynb
def mk_doctype(content:str,  # The document content
           src:Optional[str]=None # URL, filename, etc; defaults to `md5(content)` if not provided
          ) -> namedtuple:
    "Create a `doctype` named tuple"
    if src is None: src = hashlib.md5(content.encode()).hexdigest()[:8]
    return doctype(_add_nls(str(src).strip()), _add_nls(content.strip()))

# %% ../00_xml.ipynb
def mk_doc(index:int,  # The document index
           content:str,  # The document content
           src:Optional[str]=None, # URL, filename, etc; defaults to `md5(content)` if not provided
           **kwargs
          ) -> tuple:
    "Create an `ft` format tuple for a single doc in Anthropic's recommended format"
    dt = mk_doctype(content, src)
    content = Document_content(NotStr(dt.content))
    src = Src(NotStr(dt.src))
    return Document(src, content, index=index, **kwargs)

# %% ../00_xml.ipynb
def docs_xml(docs:list[str],  # The content of each document
             srcs:Optional[list]=None,  # URLs, filenames, etc; each one defaults to `md5(content)` if not provided
             prefix:bool=False, # Include Anthropic's suggested prose intro?
             details:Optional[list]=None, # Optional list of dicts with additional attrs for each doc
             title:str=None # Optional title attr for Documents element
            )->str:
    "Create an XML string containing `docs` in Anthropic's recommended format"
    pre = 'Here are some documents for you to reference for your task:\n\n' if prefix else ''
    if srcs is None: srcs = [None]*len(docs)
    if details is None: details = [{}]*len(docs)
    docs = (mk_doc(i+1, d, s, **kw) for i,(d,s,kw) in enumerate(zip(docs,srcs,details)))
    kw = dict(title=title) if title else {}
    return pre + to_xml(Documents(*docs, **kw), do_escape=False)

# %% ../00_xml.ipynb
def read_file(fname, out=True, max_size=None):
    "Read file content, converting notebooks to XML if needed"
    fname = Path(fname)
    if fname.suffix == '.ipynb': res = nb2xml(fname, out=out)
    else: res = fname.read_text()
    if max_size and len(res)>max_size: return f"[Skipped: {fname.name} exceeds {max_size} bytes]"
    return res

# %% ../00_xml.ipynb
@delegates(docs_xml)
def files2ctx(
    fnames:list[Union[str,Path]], # List of file names to add to context
    out:bool=True, # Include notebook cell outputs?
    srcs:Optional[list]=None, # Use the labels instead of `fnames`
    max_size:int=None, # Skip files larger than this (bytes)
    **kwargs
)->str: # XML for LM context
    "Convert files to XML context, handling notebooks"
    fnames = [Path(o) for o in fnames]
    contents = [read_file(o, out=out, max_size=max_size) for o in fnames]
    return docs_xml(contents, srcs or fnames, **kwargs)

# %% ../00_xml.ipynb
@delegates(globtastic)
def folder2ctx(
    folder:Union[str,Path], # Folder to read
    prefix:bool=False, # Include Anthropic's suggested prose intro?
    out:bool=True, # Include notebook cell outputs?
    include_base:bool=True, # Include full path in src?
    title:str=None, # Optional title attr for Documents element
    max_size:int=100_000, # Skip files larger than this (bytes)
    max_total:int=10_000_000,  # Max total output size in bytes
    readme_first:bool=False,  # Prioritize README files at start of context?
    files_only:bool=False,  # Return dict of {filename: size} instead of context?
    **kwargs
)->Union[str,dict]:
    "Convert folder contents to XML context, handling notebooks"
    folder = Path(folder)
    fnames = pglob(folder, **kwargs)
    if files_only: return {str(f.relative_to(folder)): f.stat().st_size for f in fnames}
    if readme_first: fnames = sorted(fnames, key=lambda f: (0 if 'readme' in f.name.lower() else 1, f))
    srcs = fnames if include_base else [f.relative_to(folder) for f in fnames]
    res = files2ctx(fnames, prefix=prefix, out=out, srcs=srcs, title=title, max_size=max_size)
    suf = f"\n\n[TRUNCATED: output size {{_outsz_}} exceeded max size {max_total} bytes]"
    if max_total and len(res) > max_total: res = truncstr(res, max_total, suf=suf, sizevar='_outsz_')
    return res

# %% ../00_xml.ipynb
def sym2file(sym):
    "Return md string with filepath and contents for a symbol's source file"
    f = Path(inspect.getfile(sym))
    return f"- `{f}`\n\n````\n{f.read_text()}\n````"

# %% ../00_xml.ipynb
@delegates(folder2ctx)
def sym2folderctx(
    sym,
    types:str|list='py',  # list or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg
    skip_file_re=r'^_mod',
    **kwargs):
    "Return folder context for a symbol's source file location"
    return folder2ctx(Path(inspect.getfile(sym)).parent, types=types, skip_file_re=skip_file_re, **kwargs)

# %% ../00_xml.ipynb
def sym2pkgpath(sym):
    "Get root package path for a symbol"
    root = sym.__module__.split('.')[0]
    return Path(sys.modules[root].__path__[0])

# %% ../00_xml.ipynb
@delegates(folder2ctx)
def sym2pkgctx(sym, types:str|list='py', skip_file_re=r'^_mod', **kwargs):
    "Return repo context for a symbol's root package"
    return folder2ctx(sym2pkgpath(sym), types=types, skip_file_re=skip_file_re, **kwargs)

# %% ../00_xml.ipynb
@call_parse
@delegates(folder2ctx)
def folder2ctx_cli(
    folder:str, # Folder name containing files to add to context
    out:bool=True, # Include notebook cell outputs?
    **kwargs # Passed to `folder2ctx`
)->str: # XML for Claude context
    "CLI to convert folder contents to XML context, handling notebooks"
    print(folder2ctx(folder, out=out, **kwargs))

# %% ../00_xml.ipynb
def parse_gh_url(url):
    "Parse GitHub URL into (owner, repo, type, ref, path) or None"
    m = re.match(r'https?://(?:www\.)?github\.com/([^/]+)/([^/]+)(?:/([^/]+)(?:/([^/]+)(?:/(.+))?)?)?', url)
    return dict(zip('owner repo typ ref path'.split(), m.groups())) if m else None

# %% ../00_xml.ipynb
@delegates(folder2ctx)
def repo2ctx(
    owner:str,  # GitHub repo owner or "owner/repo" or a full github URL
    repo:str=None,   # GitHub repo name (leave empty if using "owner/repo" or URL format for owner param)
    ref:str=None,  # Git ref (branch/tag/sha) (get from URL not provided); defaults to repo's default branch
    folder:str=None,  # Only include files under this path (get from URL not provided)
    show_filters:bool=True,  # Include filter info in title?
    token:str=None,  # GitHub token (uses GITHUB_TOKEN env var if None)
    **kwargs  # Passed to `folder2ctx`
)->Union[str,dict]:  # XML for LM context, or dict of file sizes
    "Convert GitHub repo to XML context without cloning"
    import tempfile, tarfile, io
    if owner.startswith('http'):
        parsed = parse_gh_url(owner)
        if not parsed: raise ValueError(f"Invalid GitHub URL: {owner}")
        owner,repo = parsed['owner'], parsed['repo']
        ref = ref or parsed.get('ref')
        folder = folder or parsed.get('path')
    if repo is None: owner, repo = owner.split('/')
    api = GhApi(token=token)
    if ref is None: ref = api.repos.get(owner, repo).default_branch
    data = api.repos.download_tarball_archive(owner, repo, ref)
    title = f"GitHub repository contents from {owner}/{repo}/{ref}"
    if folder: title += f'/{folder}'
    if show_filters:
        parts = [f"{k}: {', '.join(v) if isinstance(v, (list,tuple)) else v}" for k,v in kwargs.items() if v]
        if parts: title += f" (filters applied -- {' | '.join(parts)})"
    tf = tarfile.open(fileobj=io.BytesIO(data))
    with tempfile.TemporaryDirectory() as tmp:
        tf.extractall(tmp, filter='data')
        subdir = Path(tmp) / tf.getmembers()[0].name.split('/')[0]
        if folder: subdir = subdir/folder
        return folder2ctx(subdir, include_base=False, title=title, readme_first=True, **kwargs)
